# -*- coding: utf-8 -*-
"""st.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hLa695hxknNKZTWaZk7JRITo3ZUcUQYY
"""

!pip install streamlit replicate langchain -q
!pip install gdown -U --no-cache-dir -q

import streamlit as st
import replicate, os, gdown
from langchain_community.llms import Replicate
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS
from langchain import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory


# App title
st.set_page_config(page_title="Vyzer")
# ------------------------------------------------------------------------------
# Replicate Credentials
# with st.sidebar:
    # st.title('ü¶ôüí¨ Llama 2 Chatbot')
    # if 'REPLICATE_API_TOKEN' in st.secrets:
        # st.success('API key already provided!', icon='‚úÖ')
    # replicate_api = 'r8_32rGkefbyNoWO9MIvnXVCP4KO0yWVIa1vhxJY'
        # replicate_api = st.secrets['REPLICATE_API_TOKEN']
    # else:
        # replicate_api = st.text_input('Enter Replicate API token:', type='password')
        # if not (replicate_api.startswith('r8_') and len(replicate_api)==40):
        #     st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')
        # else:
        #     st.success('Proceed to entering your prompt message!', icon='üëâ')
    # os.environ['REPLICATE_API_TOKEN'] = replicate_api

    # llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'
    # llm = Replicate(
        # model= "a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5",
        # model_kwargs= {"temperature":0.0, "max_length": 512, "top_p": 1},
    # )
# ------------------------------------------------------------------------------
# Replicate Credentials
with st.sidebar:
    st.title('ü¶ôüí¨ Llama 2 Chatbot')
    # if 'REPLICATE_API_TOKEN' in st.secrets:
        # st.success('API key already provided!', icon='‚úÖ')

    replicate_api = 'r8_32rGkefbyNoWO9MIvnXVCP4KO0yWVIa1vhxJY'
        # replicate_api = st.secrets['REPLICATE_API_TOKEN']
    # else:
        # replicate_api = st.text_input('Enter Replicate API token:', type='password')
        # if not (replicate_api.startswith('r8_') and len(replicate_api)==40):
            # st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')
        # else:
            # st.success('Proceed to entering your prompt message!', icon='üëâ')
os.environ['REPLICATE_API_TOKEN'] = replicate_api
chat_history = []

# llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'
llm = Replicate(
    model= "a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5",
    model_kwargs= {"temperature": 0.0, "max_length": 800, "top_p": 1},
)


# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "Welcome, How may I assist you today?"}]
    print(st.session_state.messages)

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    chat_history = []
    st.session_state.messages = [{"role": "assistant", "content": "Yesss, How may I assist you today?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# Function for generating LLaMA2 response.
def generate_llama2_response(prompt_input):
    # string_dialogue = """
    #   Be humble and polite.
    #   Don't try to make up an answer, if you don't know just say that you don't know.
    #   Answer in the same language the question was asked.
    #   If the following context is not related to the question, skip it and answer by your own, but it should be legit.
    # # """
    # for dict_message in st.session_state.messages:
    #     if dict_message["role"] == "user":
    #         string_dialogue += "User: " + dict_message["content"] + "\n\n"
    #     else:
    #         string_dialogue += "Assistant: " + dict_message["content"] + "\n\n"
    output = gen_output(prompt_input)
    return output

def gen_output(prompt_input):

    # Loading the FAISS index
    url = 'https://drive.google.com/drive/folders/1VeHYh2hqKd_p_AqcWrb74iutPhpnje3r'
    gdown.download_folder(url, quiet=True)

    vectordb = FAISS.load_local("faiss_index", HuggingFaceEmbeddings())

    prompt_template = """
    Be humble and polite.
    Don't try to make up an answer, if you don't know just say that you don't know.
    Answer in the same language the question was asked.
    If the following context is not related to the question, skip it and answer by your own, but it should be legit.

    {context}

    {history}

    Question: {question}
    Answer:"""

    PROMPT = PromptTemplate(
      template= prompt_template,
      input_variables=["history","context", "question"])

    chain = RetrievalQA.from_chain_type(llm= llm,
                                      chain_type="stuff",
                                      retriever= vectordb.as_retriever(),
                                      chain_type_kwargs= {'prompt': PROMPT,
                                                          "verbose": True,
                                                          "memory": ConversationBufferMemory(
                                                                    memory_key="history",
                                                                    input_key="question"),
                                                          },
                                      return_source_documents= True,
                                      verbose= True)

    result = chain({'question: ': f"{prompt_input}", 'chat_history': chat_history})
    chat_history.append((prompt_input, result['answer']))

    return result['answer']

# User-provided prompt
if prompt := st.chat_input(disabled=not replicate_api):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_llama2_response(prompt)
            placeholder = st.empty()
            full_response = ''
            for item in response:
                full_response += item
                placeholder.markdown(full_response)
            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)

